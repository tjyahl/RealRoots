%Section1.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Real roots of univariate polynomials}

Let $f\in\RR[x]$ be a polynomial.
It has the form
%
 \[
   f\ =\ c_{0}x^{a_{0}} + c_{1}x^{a_{1}} + \cdots + c_{m}x^{a_{m}}\,,
 \]
%
where $a_{0} < a_{1} < \cdots < a_{m}$ are nonnegative integers and for $0\leq i \leq m$, $c_{i}$ is a nonzero real number.
Let $\defcolor{\var(c_0,\dotsc,c_m)}\vcentcolon=\#\{1\leq i\leq m\mid c_{i-1}c_i<0\}$ be the number of sign variations in the coefficients
of $f$.
Descartes' Rule of Signs \cite{So_Book} gives an upper bound for the number of positive real roots of $f$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Descartes' Rule of Signs]
  The number of positive real roots of $f$, counted with multiplicity, is at most $\var(c_0,\dotsc,c_m)$ and it has the same parity
  as $\var(c_0,\dotsc,c_m)$.
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

More generally, given any sequence $c=(c_0,\dotsc,c_m)$, we compute the \demph{variation} of $c$, $\var(c)$, by counting the number of
variations in sign after removing all zero terms.
%
\begin{leftbar}
\verbatiminput{examples/variations.txt}
\end{leftbar}
%
For sequence of polynomials  $\defcolor{f_\bullet}=(f_0,\dotsc,f_k)$ in $\RR[x]$ and $a\in\RR$, \defcolor{$\var(f_\bullet,a)$} is the
variation in the sequence 
$(f_0(a),\dotsc,f_{k}(a))$. 
We extend this to $a=\pm\infty$, by taking $f(\infty)$ to be the leading coefficient of $f(x)$ and $f(-\infty)$ to be the leading
coefficient of $f(-x)$.

Given a polynomial $f\in\RR[x]$ of degree $\ell$, consider its sequence of derivatives,
%
 \[
   \defcolor{\delta f}\ \vcentcolon= \left(f(x),d'(x),d''(x),\dots,d^{(l)}(x)\right)\,.
 \]
%
For $a,b\in \RR\cup\{\pm \infty\}$ with $a<b$, let \defcolor{$r(f,a,b)$} be the number of roots of $f$ in the interval $(a,b\hspace{.05cm}]$, counted
with multiplicity.
Budan and Fourier~\cite[Ch.\ 2]{So_Book} generalized Descartes' Rule.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Budan-Fourier]
  We have that $r(f,a,b)\leq \var(\delta f,a) -\var(\delta f,b)$, and the difference
  $\var(\delta f,a) -\var(\delta f,b)-r(f,a,b)$ is even. 
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Descartes' Rule is when $a=0$ and $b=\infty$.
Let us consider an example.
%
\begin{leftbar}
\verbatiminput{examples/BudanFourierBound.txt}
\end{leftbar}
%
When called on a polynomial, \texttt{BudanFourierBound} assumes that $a=-\infty$ and $b=\infty$, giving a bound for all real roots.
Note that $r(f,0,\infty)=4$ and $r(f,-\infty,\infty)=7$.

In contrast to these bounds, 
Sylvester's Theorem determines the actual number of real roots, and more.
The \demph{Sylvester sequence}, \defcolor{$\Syl(f,g)$} of polynomials $f,g\in\RR[x]$ is the sequence
$\left(f_0,f_1,\dotsc,f_k\right)$ of nonzero polynomials, where $f_0\vcentcolon= f, f_1 \vcentcolon= f'\cdot g$,
and for $i\geq 1$, 
%
  \[
    f_{i+1}\ \vcentcolon=\ -1\cdot \mbox{remainder}(f_{i-1},f_i)\,,
  \]
%
the negative remainder term in the division of $f_{i-1}$ by $f_i$.
Note that we have $f_k = \gcd(f,f'g)$ and for each $i=0,\dotsc,k$ there exists $q_i\in\RR[x]$ such that
%
 \begin{equation}\label{Eq:divisionAlgorithm}
    f_{i-1}\ =\ q_i(x)f_i(x)-f_{i+1}(x)\,.
 \end{equation}
%
The \demph{reduced Sylvester sequence} is obtained by dividing each term in the Sylvester sequence by the $\gcd$ $f_k$.
Note that the elements $(g_0,\dotsc,g_k)$ of the reduced sequence satisfy~\eqref{Eq:divisionAlgorithm} with $f_j$ replacing $g_j$, and
we also have $g_k=1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Sylvester]
  \label{Th:Sylvester}
  Let $f,g\in\RR[x]$ and suppose that $g_\bullet$ is the reduced Sylvester sequence of $f$ and $f'g$.
  For $a<b$ in $\RR\cup\{\pm\infty\}$ we have,
  %
  \begin{multline*}
    \qquad\var(g_\bullet,a)-\var(g_\bullet,b)\ =\
    \#\{\zeta\in(a,b\hspace{.05cm}]\mid f(\zeta)=0\mbox{ and } g(\zeta)>0\}\\
      \ -\
    \#\{\zeta\in[a,b)\mid f(\zeta)=0\mbox{ and } g(\zeta)<0\}\,.\qquad
  \end{multline*}
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
  In~\cite{BCR}\footnote{Give a more precise reference}, this is stated and proven in the case when $f$ does not vanish at $a$ or at $b$,
  for the Sylvester sequence $\Syl(f,f'g)$, and not for the reduced Syvester sequence.
  The proof proceeds by studying the variation $\var(\Syl(f,g), t)$ as $t$ increases fron $a$ to $b$, noting that this quantity may only
  change when $t$ passes a root of some member of the Sylvester sequence $\Syl(f,g)$.
  Since multiplying a sequence by a nonzero number $f_(kt)$ does not change its variation, the proof in~\cite{BCR} establishes the theorem
  when $f$ does not vanish at $a$ or at $b$.

  The variation $\var(g_\bullet,t)$ may only change when $t$ passes a root $\zeta$ of some polynomial $g_i$ in the reduced Sylvester
  sequence $g_\bullet$, or possibly when the
  root $\zeta$ is an   endpoint of $[a,b\hspace{.05cm}]$.
  Observe that $\zeta$ cannot be a root of two consecutive elements of $g_\bullet$.
  If it were, then by~\eqref{Eq:divisionAlgorithm} and an induction, it is a root of all elements of $g_\bullet$, and thus of $g_k=1$, which is a
  contradiction.
  Suppose that $g_i(\zeta)=0$ for some $i\geq 1$.
  By~\eqref{Eq:divisionAlgorithm} again $g_{i-1}(\zeta)$ and $g_{i+1}(\zeta)$ have opposite signs in a neighborhood of $\zeta$ and thus
  $g_{i-1},g_i,g_{i+1}$ do not contribute to any change in $\var(G,t)$ for $t$ in a neighborhood of $\zeta$.
  This in particular remains true if $\zeta=a$ and $t$ increases from $a$ or if $\zeta=b$ and $t$ approaches $b$.

  We now suppose that $g_0(\zeta)=0$ and thus $g_1(\zeta)\neq 0$.
  Then we have $f(\zeta)=0$.
  Let $m$ be the multiplicity of the root $\zeta$ of $f$.
  Then $f=(x-\zeta)^m h$ with $h(\zeta)\neq 0$.
  If $g(\zeta)=0$, then $(x-\zeta)^m$ divides $f'g$ and thus $f_k$, and so $g_0=f/f_k$ does not vanish at $\zeta$.
  Thus $g(\zeta)\neq 0$.

  Notice that $h_0\vcentcolon=f/(x-\zeta)^{m-1}$ and $h_1\vcentcolon=f'g/(x-\zeta)^{m-1}$ have the same signs in a neighborhood of $\zeta$
  as do $g_0$ and $g_1$.
  A computation reveals that $h_1=mhg+(x{-}\zeta)h'g$. 
 Let $\epsilon>0$ be a real number such that $\zeta$ is the only root if any element in $H$ lying in the interval
 $[\zeta-\epsilon,\zeta+\epsilon]$.
 We have
 %
 \[
 \begin{array}{c|c|l}
   x & h_0(x) & h_1(x)\\\hline
   \zeta-\epsilon & -\epsilon h(\zeta-\epsilon)  & mh(\zeta-\epsilon)g(\zeta-\epsilon) - \epsilon h'(\zeta-\epsilon)g(\zeta-\epsilon)\\
   \zeta     &     0    &   mh(\zeta)g(\zeta)\\
   \zeta+\epsilon & \epsilon h(\zeta-\epsilon)  & mh(\zeta-\epsilon)g(\zeta-\epsilon) + \epsilon h'(\zeta-\epsilon)g(\zeta-\epsilon)\\
 \end{array}
 \]
 %
  
 Let $\epsilon>0$ be a real number such that $\zeta$ is the only root if any element in $H$ lying in the interval
 $[\zeta-\epsilon,\zeta+\epsilon]$.
 We have
 %
 \[
 \begin{array}{c|c|l}
   x & h_0(x) & h_1(x)\\\hline
   \zeta-\epsilon & -\epsilon h(\zeta-\epsilon)  & mh(\zeta-\epsilon)g(\zeta-\epsilon) - \epsilon h'(\zeta-\epsilon)g(\zeta-\epsilon)\\
   \zeta     &     0    &   mh(\zeta)g(\zeta)\\
   \zeta+\epsilon & \epsilon h(\zeta-\epsilon)  & mh(\zeta-\epsilon)g(\zeta-\epsilon) + \epsilon h'(\zeta-\epsilon)g(\zeta-\epsilon)\\
 \end{array}
 \]
 %

 Suppose that $g(\zeta)>0$.
 Then the sign of $h_1$ on $[\zeta-\epsilon,\zeta+\epsilon]$ is opposite to the sign of $h_0(\zeta-\epsilon)$, but the same as the sign of
 $h_0(\zeta+\epsilon)$.
 We conclude that the variation $\var(H,t)=\var(G,t)$ decreases by 1 as $t$ passes from $\zeta-\epsilon$ to $\zeta$, but is unchanged as $t$
 passes from $\zeta$ to $\zeta+\epsilon$,
 \[
 \var(G,\zeta-\epsilon)-1\ =\ \var(G,\zeta)\ =\ \var(G,\zeta+\epsilon)\,.
 \]
  

 Suppose that $g(\zeta)<0$.
 Then the sign of $h_1$ on $[\zeta-\epsilon,\zeta+\epsilon]$ is the same as the sign of $h_0(\zeta-\epsilon)$, but opposite to the sign of
 $h_0(\zeta+\epsilon)$.
 We conclude that the variation $\var(H,t)=\var(G,t)$ is unchanged as $t$ passes from $\zeta-\epsilon$ to $\zeta$, but increases by 1 as $t$ 
 passes from $\zeta$ to $\zeta+\epsilon$,
 \[
   \var(G,\zeta-\epsilon)\ =\ \var(G,\zeta)\ =\ \var(G,\zeta+\epsilon)-1\,.
 \]

 Now consider the variation $\var(G,t)$ for $t\in[a,b\hspace{.05cm}]$.
 This may only change at a number $\zeta\in[a,b]$ if $f(\zeta)=0$.
 If $g(\zeta)>0$ and $\zeta\neq b$, then it decreases by 1.
 If $g(\zeta)<0$ and $\zeta\neq a$, then it increases by 1.
 It is unchanged in all other cases.
 This completes the proof of the Theorem.  
 \end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%Sturm's theorem
\corollary[Sturm's Theorem]
Let $f$ be a univariate polynomial and $a,b\in \mathbb{R}\cup\{\pm\infty\}$ with $a<b$ and $f(a),f(b)\neq 0$. Then the number of zeros of $f$ in the interval $(a,b\hspace{.05cm}]$ is the difference 
\begin{align*}
\text{var}(F,a) - \text{var}(F,b),
\end{align*}
where $F$ is the Sturm sequence of $f$.

%Real root isolation


%Hurwitz stability
\theorem
Let $f(x) = \sum_{j=0}^{n}a_{j}x^{j}$ with $n\geq 1$ and $a_{n}>0$. Then $f$ is Hurwitz stable if and only if all the Hurwitz determinants $\delta_{1},\dots,\delta_{n}$ are all positive.
%
%M2 examples along the way
%
